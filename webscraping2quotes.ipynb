{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878800e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d848819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_URL = \"https://quotes.toscrape.com\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Educational Scraper - Mukono)\"}\n",
    "OUTPUT_FILE = \"quotes_mukono.csv\"  # Change 'mukono' only if your surname differs\n",
    "DELAY = 1.5\n",
    "\n",
    "# === DATA STORAGE ===\n",
    "quotes_data = []\n",
    "\n",
    "# === SCRAPING LOOP ===\n",
    "page = 1\n",
    "while True:\n",
    "    url = f\"{BASE_URL}/page/{page}/\"\n",
    "    print(f\"Scraping page {page}: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 404:\n",
    "            print(\"Page not found — end of pagination.\")\n",
    "            break\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        break\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    quote_blocks = soup.select(\"div.quote\")\n",
    "    \n",
    "    if not quote_blocks:\n",
    "        print(\"No quotes found — likely last page.\")\n",
    "        break\n",
    "    \n",
    "    for block in quote_blocks:\n",
    "        text = block.select_one(\"span.text\").get_text(strip=True).strip('“”')\n",
    "        author = block.select_one(\"small.author\").get_text(strip=True)\n",
    "        tags = \", \".join([tag.get_text() for tag in block.select(\"div.tags a.tag\")])\n",
    "        quote_url = BASE_URL + block.find(\"a\")[\"href\"]\n",
    "        \n",
    "        quotes_data.append({\n",
    "            \"Quote\": text,\n",
    "            \"Author\": author,\n",
    "            \"Tags\": tags,\n",
    "            \"URL\": quote_url\n",
    "        })\n",
    "    \n",
    "    page += 1\n",
    "    time.sleep(DELAY)\n",
    "\n",
    "# === SAVE TO CSV ===\n",
    "if quotes_data:\n",
    "    keys = quotes_data[0].keys()\n",
    "    with open(OUTPUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(quotes_data)\n",
    "    \n",
    "    print(f\"\\nSuccess! {len(quotes_data)} quotes saved to {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\"No data collected.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
